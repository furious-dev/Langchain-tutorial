{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM powered Search Application with Ollama & LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword based Seaching & Summerization of medium articles using Langchain and ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install langchain\n",
    "%pip install langchain-google-community\n",
    "%pip install langchain-ollama\n",
    "%pip install python-dotenv\n",
    "%pip install tqdm\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variable which are needed in the notebook.\n",
    "Needed env variables: GOOGLE_CSE_ID & GOOGLE_API_KEY to connect to google api for searching medium articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function extracts metadata such as **auther details**, **photo**, **title of the article**, **followers**, **read time of the article** etc from the scrapped medium article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(soup):\n",
    "    metadata={\n",
    "        \"read_time\" : soup.select_one(\"span[data-testid=storyReadTime]\").text,\n",
    "        \"published_date\": soup.select_one(\"span[data-testid=storyPublishDate]\").text,\n",
    "        \"author\" : soup.select_one(\"a[data-testid=authorName]\").text,\n",
    "        \"title\": soup.find('meta', {\"property\": \"og:title\"})['content'],\n",
    "        \"author_url\" : soup.find('meta', {\"property\": \"article:author\"})['content'],\n",
    "        \"followers\": soup.select_one(\"span[class='pw-follower-count bf b bg z bk']\").text,\n",
    "        \"author_image\": soup.select_one(\"img[data-testid=authorPhoto]\")['src']\n",
    "    }\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell creates a prompt template using the **ChatPromptTemplate** module of LangChain fro summerizing the medium articles scrapped from the internet.<br>\n",
    "It also uses the **chaining technique of LangChain** to combine the prompt template with the LLM model.<br><br>\n",
    "Ensure that Ollama is running on your machine. If it's not running, you can start it by Ollama on the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''\n",
    "You are an expert in summerizing a article from web. \n",
    "your job to precisely summarize the given article in an easy and readable way. \n",
    "Remove any coding examples or code blocks or links in the article. \n",
    "Do not put any extra comment or try to explain anything.\n",
    "\n",
    "article:\n",
    "{input}\n",
    "\n",
    "output: \n",
    "\n",
    "'''\n",
    "promptTemplate = ChatPromptTemplate.from_template(prompt)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "chain = promptTemplate | model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below cell, will utilize **BeautifulSoup** to parse the URLs obtained from the search. <br><br> The code scrapes Medium URLs from Google search results and extracts metadata such as author details, title, follower count, read time, etc. It also generates a summary of the article using a locally running **LLaMA** model via **Ollama**. <br><br>Finally, all extracted data and the summary are compiled into a structured JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(urls):\n",
    "    extracted_response = []\n",
    "    with tqdm(total=len(urls)) as pbar:\n",
    "        for url in urls:\n",
    "            headers = {\n",
    "                    \"User-Agent\": \"Guest\"\n",
    "                }\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:    # if request granted\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            meatdata = extract_metadata(soup)\n",
    "            meatdata[\"summary\"] = chain.invoke({\"input\":soup.get_text()})\n",
    "            extracted_response.append(meatdata)\n",
    "            pbar.update(1)\n",
    "    return extracted_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell uses the **GoogleSearchAPIWrapper** feature, which is a LangChain wrapper for performing Google searches. Simply send the query you want to search, and it will return the snippet, title, and link information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(query, count=5):\n",
    "    search = GoogleSearchAPIWrapper()\n",
    "\n",
    "    def search_results(query):\n",
    "        return search.results(query,count)\n",
    "\n",
    "    tool = Tool(\n",
    "        name=\"google_search\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=search_results\n",
    "    )\n",
    "    results = tool.run(query)\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "    urls = [article[\"link\"] for article in results] \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it's time to run the application and review the resultsÂ !!!!!!\n",
    "\n",
    "Make sure to have \"site:medium.com\" before the actual search keyword. This makes sure that it searches articles only on the medium site. Other it will search over entire Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"site:medium.com GenAI\"\n",
    "\n",
    "\n",
    "print(\"\\n\\nSearching Urls ......\")\n",
    "urls = get_search_results(query)\n",
    "\n",
    "print(\"\\n\\nExtrating & summerizing the Urls ......\")\n",
    "response = extract_data(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the results in pandas dataframe to visualize the results in structures tabular format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the summary generated by the local llama 3.2 llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurray!!!!!!! we have sucessfully build an medium application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
